input {
	elasticsearch {
		hosts => "localhost"
		index => "muzo-cgs-objednavka"
		#query => '{ "query": { "query_string": { "query": "*" } } }'
		query => '{"query":{"match":{"version":"2"}},"sort":[{"_id":{"order":"asc"}}]}'
		size => 10000
		scroll => "5m"
		docinfo => true
	}
}

filter {
	ruby {
		init => "
			begin
				tday = Time.now.strftime('%Y-%m-%d')
				@@csv_stamp = tday
				@@csv_file    = '/var/ftp/pub/cgs-objednavky_' + tday + '.csv'
				@@csv_headers = ['cislo_objednavky', 'popis', 'dodavatel', 'ico', 'datum_objednani', 'datum_dodani', 'celkova_castka', 'mena']
				if File.zero?(@@csv_file) || !File.exist?(@@csv_file)
			 		CSV.open(@@csv_file, 'w', {:col_sep => ','}) do |csv|
						csv << @@csv_headers
			  		end
				end
    		end
  		"
    	code => "
			begin
				event.set('[@metadata][csv_file]', @@csv_file)
				event.set('[@metadata][csv_headers]', @@csv_headers)
				event.set('[filepath]', @@csv_file)
				event.set('[csvstamp]', @@csv_stamp)
				datearr1 = ['dateconclusion','datevalidity']
				datearr2 = ['dateconclusionStr','datevalidityStr']	    
				for i in 0..1
					event.set(datearr2.at(i), '')
					splitDate = ''
					if event.get(datearr1.at(i)) != nil
						tstamp = event.get(datearr1.at(i))
						event.set(datearr2.at(i), tstamp[0..9])
					end
				end
    		end
    	"
	}
	mutate {
		add_field => {
	    	"mena" => "CZK"
		}
		convert => { 
	    	"total" => "float"
		}
	}
}

output {
	csv {
		spreadsheet_safe => true
		fields => ["orderid", "title", "contractorname", "contractorid", "dateconclusionStr", "datevalidityStr", "total", "mena"]
		csv_options => {"col_sep" => ","}
		path => "/var/ftp/pub/cgs-objednavky_%{csvstamp}.csv"
	}
	stdout {
		codec => dots
	}
}
